---
title: "A Corpus-Based Acoustic Analysis of Monophthong Vowels among Chinese Learners and Native Speakers of English - Part 3"
author: "Martin Schweinberger"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: html_document
---


# Introduction



# Preparation

install packages

```{r install, eval=F, message=F, warning=F}
# install
install.packages("tidyverse")
install.packages("here")
install.packages("adehabitatHR")
install.packages("lme4")
install.packages("sjPlot")
install.packages("report")
install.packages("flextable")
install.packages("cowplot")     
install.packages("randomForest") 
install.packages("rms") 
install.packages("caret") 
install.packages("Hmisc") 
install.packages("glmulti") 
install.packages("partykit") 
install.packages("ggparty")
install.packages("hunspell")
install.packages("janitor")
install.packages("glmmTMB")
install.packages("quanteda")
```

load packages

```{r load, message=F, warning=F}
library(tidyverse)
library(here)
library(adehabitatHR)
library(lme4)
library(sjPlot)
library(report)
library(flextable)
library(cowplot)      
library(randomForest) 
library(rms)    
library(caret) 
library(Hmisc) 
library(quanteda)  
#library(glmulti) 
library(partykit)   
library(ggparty)
library(hunspell)
library(janitor)
# set options
options(stringsAsFactors = F)                           
options(scipen = 999) 
options(max.print=10000)
```


# Load data


```{r data, message=F, warning=F}
# load .rda data
cdat  <- base::readRDS(file = here::here("data", "cleandat.rda")) %>%
  dplyr::ungroup()
# inspect
str(cdat); head(cdat)
```






# Reduce data

```{r redux, message=F, warning=F}
bdat <- cdat %>%
  dplyr::mutate(label = stringr::str_remove_all(label, ":"),
                gender = ifelse(gender == "f", "female", gender),
                gender = ifelse(gender == "m", "male", gender),
                tvariety = ifelse(tvariety == "us", "AmE", tvariety),
                tvariety = ifelse(tvariety == "gb", "BrE", tvariety)) %>%
#  dplyr::filter(label != "A",
#                label != "O") %>%
  droplevels(.)  %>%
  dplyr::rename(Vowel = label,
                Word = word,
                TargetVariety = tvariety,
                Gender = gender,
                Duration = duration,
                Proficiency = prof,
                Speaker = speaker) %>%
  # clean word
  dplyr::mutate(Word = str_remove_all(Word, "\\W")) %>%
  dplyr::filter(hunspell_check(Word) == T) %>%
  # remove "shits"
  dplyr::filter(Word != "shits",
                Word != "stat",
                Word != "whats")
# inspect
head(bdat); names(table(bdat$Word))
```


# Check frequency of words

```{r}
# create a vector of words
words <- names(table(bdat$Word))
# load ace files
afiles <- list.files(here::here("ACE"), pattern = ".TXT", recursive = T, full.names = T) 
bfiles <- list.files(here::here("BROWN"), pattern = ".TXT", recursive = T, full.names = T) 
lfiles <- list.files(here::here("LOB"), pattern = ".TXT", recursive = T, full.names = T) 
cfiles <- c(afiles, bfiles, lfiles)
```


```{r}
# load the files by scanning the content
controlc <- sapply(cfiles, function(x){
  x <- scan(x, what = "char",  sep = "", quote = "",  quiet = T,  skipNul = T)
  x <- paste0(x, sep = " ", collapse = " ")
  x <- stringr::str_squish(x)
})
controlc <- paste0(controlc, collapse = " ")
# inspect
str(controlc)
```

extract word count of control corpus

```{r}
cleancontrolc <- controlc %>%
  stringr::str_replace_all("<.*?>", " ") %>%
  stringr::str_replace_all("[^[:alpha:] ]", " ") %>%
  stringr::str_squish() %>%
  quanteda::tokenize_fastestword() %>%
  unlist() %>%
  length()
# inspect
cleancontrolc
```

check how frequent the words are in the control corpus

```{r}
freqs <- sapply(words, function(x){
  x <- stringr::str_count(controlc, paste0("\\W", x, "\\W", sep = "", collapse = ""))
})
# convert into data frame
freqsdf <- data.frame(names(freqs), freqs, cleancontrolc) %>%
  dplyr::rename(Word = 1,
                all = 3) %>%
  dplyr::mutate(Frequency = log(freqs/all*1000)) %>%
  dplyr::select(-freqs, -all)
# inspect
head(freqsdf)
```


## Annotate word class

```{r}
lexical <- c("bad",  "bed", "best", "big", "bit", "book", "books", "boost", "boots", "boss", "bought", "buds", "bus", "butts", "dad", "dead", "death", "debt", "debts", "desk", "dish",  "dust", "gap", "gas",  "good",  "guess", "head", "heads",  "hit", "hot", "key", "kid", "kids", "pass", "past", "pat", "path", "pub", "pubs", "push", "sad", "said", "sat", "says", "seat", "seats", "see", "seep", "sees", "set", "sets",  "shits", "shoes", "shop", "shops", "shut", "sit", "skip",  "speak", "spots", "stat", "step", "steps", "stop", "stops", "stud", "suit", "task", "tasks", "tea", "teeth", "test", "tests", "took", "top", "tough", "two", "wash", "ways",  "weak", "weed", "week",  "wish",  "wood")
bdat <- bdat %>%
  dplyr::mutate(WordClass = ifelse(Word %in% lexical, "lexical", "grammatical"),
                Word = as.vector(Word))
bdat <- left_join(bdat, freqsdf, by = "Word")
# inspect
table(bdat$WordClass); head(bdat)
```


## Check durations

```{r}
bdat %>%
  dplyr::mutate(Vowel = dplyr::case_when(Vowel == "{" ~ "\u00E6",
                                         Vowel == "6" ~ "\u0250",
                                         Vowel == "e" ~ "\u0065",
                                         Vowel == "E" ~ "\u025B",
                                         Vowel == "i" ~ "\u0069",
                                         Vowel == "I" ~ "\u026A",
                                         Vowel == "Q" ~ "\u0252",
                                         Vowel == "u" ~ "\u0075",
                                         Vowel == "U" ~ "\u028A",
                                         Vowel == "V" ~ "\u028C",
                                         TRUE ~ Vowel))  %>%
  ggplot(aes(x = Vowel, y = Duration)) +
  geom_boxplot()
```

Remove items with exaggerated duration

```{r}
nrow(bdat)
bdat <- bdat  %>%
  # remove rare words
  dplyr::group_by(type, Word) %>%
  dplyr::mutate(freq = n()) %>%
#  dplyr::mutate(Word = ifelse(freq > 10, Word, "other")) %>%
  dplyr::ungroup()
# harmonize words
nnwords <- bdat %>%
  dplyr::filter(type == "CHN") %>%
  dplyr::group_by(Word) %>%
  dplyr::summarise(Freq = n()) %>%
  dplyr::pull(Word)

# remove rare vowels
bdat <- bdat %>%
  dplyr::group_by(Vowel) %>%
  dplyr::mutate(fr = n()) %>%
  dplyr::filter(fr > 100) %>%
  dplyr::select(-fr) %>%
  dplyr::ungroup()
# inspect
str(bdat); nrow(bdat)
```

```{r}
bdat %>%
  dplyr::mutate(Vowel = dplyr::case_when(Vowel == "{" ~ "\u00E6",
                                         Vowel == "6" ~ "\u0250",
                                         Vowel == "e" ~ "\u0065",
                                         Vowel == "E" ~ "\u025B",
                                         Vowel == "i" ~ "\u0069",
                                         Vowel == "I" ~ "\u026A",
                                         Vowel == "Q" ~ "\u0252",
                                         Vowel == "u" ~ "\u0075",
                                         Vowel == "U" ~ "\u028A",
                                         Vowel == "V" ~ "\u028C",
                                         TRUE ~ Vowel))  %>%
  ggplot(aes(x = Vowel, y = Duration)) +
  geom_boxplot()
```


```{r}

tb2 <- bdat %>%
  dplyr::ungroup() %>%
  dplyr::mutate(Age = dplyr::case_when(Age < 30 ~ "18-29",
                                       Age < 40 ~ "30-39",
                                       Age < 50 ~ "40-49",
                                       Age > 49 ~ "50+",
                                       TRUE ~ "unknown")) %>%
  dplyr::group_by(type, Gender, Age) %>%
  dplyr::summarise(speakers = length(table(Speaker))) %>%
  tidyr::spread(Age, speakers) %>%
  dplyr::ungroup()  %>%
  adorn_totals("row")%>%
  adorn_totals("col")
# save
write.table(tb2, here::here("tables", "tb2_icame.txt"), sep = "\t")
# inspect
tb2
```

```{r}
bdat <- bdat %>%
  dplyr::mutate(F1 = as.vector(scale(F1)),
                F2 = as.vector(scale(F2)),
                Duration = as.vector(scale(Duration)),
                Age = as.vector(scale(Age)))
```



# Split data

```{r clean}
nsd <- bdat %>%
  dplyr::filter(type == "ENS") %>%
  dplyr::select(-type, -Proficiency, -Speaker, -file)  %>%
  dplyr::mutate(Word = ifelse(Word %in% nnwords, Word, "other"))%>%
  dplyr::mutate_if(is.character, factor)
# inspect
head(nsd); str(nsd)
```




Remove impossible variables (too many levels)
 
```{r removeimp}
nrow(nsd)
nsd <- nsd %>%
  dplyr::select(-fspeaker)
str(nsd); nrow(nsd)
```


## Split native speaker data into test and training set

```{r}
# add id to data
nsd <- nsd %>% dplyr::mutate(id = 1:nrow(.))
# create training set (70%)
nsdtrain <- nsd %>% dplyr::sample_frac(0.70)
# create test set (30%)
nsdtest  <- dplyr::anti_join(nsd, nsdtrain, by = 'id') %>%
  dplyr::select(-id)
# remove id column
nsdtrain <- nsdtrain %>% dplyr::select(-id)
# inspect
head(nsdtrain); head(nsdtest)
```

```{r l2amp_03_43,  message=FALSE, warning=FALSE}
nnsd <- bdat %>%
  dplyr::filter(type != "ENS") %>%
  droplevels() %>%
  dplyr::select(-file, -type, -fspeaker)
# save predictors associated with proficiency for later
pred_nns <- nnsd %>% dplyr::select(Speaker, Proficiency)
# remove proficiency variables (for now)
nnsd <- nnsd %>%
  dplyr::select(-Proficiency, -Speaker)
# inspect data
head(nnsd); str(nnsd)
```

# Harmonize words

```{r}
nswords <- nsdtrain %>%
  dplyr::group_by(Word) %>%
  dplyr::summarise(Freq = n()) %>%
  dplyr::pull(Word)
nnsd <- nnsd %>%
  dplyr::mutate(Word = ifelse(Word %in% nswords, Word, "other")) %>%
  dplyr::mutate_if(is.character, factor)
# inspect
str(nnsd); str(nsdtrain)
```


Remove superfluous predictors

```{r}
# nnsd
nnsd <- nnsd %>%
  dplyr::select(-id, -Vowel, -TargetVariety, -edist, -barkF1, -barkF2, -lobF1, 
                -lobF2, -normF1, -normF2, -cF1, -cF2, -ED, -WordType, -freq) %>%
  dplyr::rename(Vowel = vowel)
# nsdtrain
nsdtrain <- nsdtrain %>%
  dplyr::select(-Vowel, -TargetVariety, -edist, -barkF1, -barkF2, -lobF1, 
                -lobF2, -normF1, -normF2, -cF1, -cF2, -ED, -WordType, -freq) %>%
  dplyr::rename(Vowel = vowel)
# nsdtest
nsdtest <- nsdtest %>%
  dplyr::select(-Vowel, -TargetVariety, -edist, -barkF1, -barkF2, -lobF1, 
                -lobF2, -normF1, -normF2, -cF1, -cF2, -ED, -WordType, -freq) %>%
  dplyr::rename(Vowel = vowel)
# inspect
colnames(nnsd); colnames(nsdtrain); colnames(nsdtest)
```

# Mixed-Model

Prepare data

```{r}
mdat <- bdat %>%
  dplyr::filter(Vowel == "i"|Vowel == "I"|Vowel == "u"|Vowel == "U"|Vowel == "E"|Vowel == "{") %>%
  dplyr::select(-Vowel, -TargetVariety, -edist, -barkF1, -barkF2, -lobF1, 
                -lobF2, -normF1, -normF2, -cF1, -cF2, -ED, -WordType, -freq, -file, -id, -fspeaker) %>%
  dplyr::rename(Vowel = vowel) %>%
  dplyr::mutate(Word = ifelse(Word %in% nswords, Word, "other")) %>%
  dplyr::mutate_if(is.character, factor)
  # inspect
str(mdat)
```

Baseline model


```{r,  message=FALSE, warning=FALSE}
# set options
options(contrasts  =c("contr.treatment", "contr.poly"))
mdat.dist <- datadist(mdat)
options(datadist = "mdat.dist")
# generate initial minimal regression model 
# baseline model glm
ma = glmer(Duration ~ (1 | Word) + (1|Speaker), family = gaussian, data = mdat) 
# inspect results
summary(ma)
# inspect 
sjPlot::tab_model(ma)
```



Model fitting

```{r}
# wrapper function for linear mixed-models
glmer.glmulti <- function(formula, data, random="",...){
  lmer(paste(deparse(formula),random),  data=data, ...)
}
# define formular
form_glmulti = as.formula(paste("Duration ~  Vowel + type +  Gender + WordClass"))
```

Extract best 5 models.

```{r eval = F}
library(glmulti)
# multi selection for glmer
mfit <- glmulti(form_glmulti, random="+(1|Speaker)+(1|Word)", 
                data = mdat, method = "h", fitfunc = glmer.glmulti,  includeobjects = T,
                crit = "aic", intercept = TRUE, marginality = FALSE, level = 2)
```

After 50 models:
Best model: Duration~1+Vowel+type+Gender+type:Vowel+Gender:Vowel
Best model: Duration~1+Vowel+type+Gender+type:Vowel+Gender:Vowel+Gender:type
Crit= 15875.9439868615
Mean crit= 16152.5991758241

After 100 models:
Best model: Duration~1+Vowel+type+Gender+WordClass+Gender:Vowel
Best model: Duration~1+Vowel+type+Gender+WordClass+Gender:Vowel+WordClass:Vowel
Best model: Duration~1+Vowel+type+Gender+WordClass+Gender:Vowel+WordClass:type
Best model: Duration~1+Vowel+type+Gender+WordClass+Gender:Vowel+WordClass:Vowel+WordClass:type
Best model: Duration~1+Vowel+type+Gender+WordClass+Gender:Vowel+WordClass:Gender
Best model: Duration~1+Vowel+type+Gender+WordClass+Gender:Vowel+WordClass:Vowel+WordClass:Gender
Best model: Duration~1+Vowel+type+Gender+WordClass+Gender:Vowel+WordClass:type+WordClass:Gender
Best model: Duration~1+Vowel+type+Gender+WordClass+Gender:Vowel+WordClass:Vowel+WordClass:type+WordClass:Gender
Crit= 15869.1516948654
Mean crit= 16099.4464548991
Completed.


```{r}
mb <- lmer(Duration ~ (1 | Word) + (1|Speaker) +
             type + Vowel + Gender + WordClass + Gender:Vowel,
           data = mdat)
# inspect 
sjPlot::tab_model(ma, mb)
```




Visualize effects
 

```{r}
sjPlot::plot_model(mb, type = "pred", terms = c("Vowel", "type")) +
  scale_color_manual(values = c("lightgray", "orange")) +
  theme_bw() +
  labs(title = "", y = "Predicted duration", x = "Speaker type")
ggsave2(here::here("images", "lmer_type.png"), width = 4, height = 3)
```

```{r}
sjPlot::plot_model(mb, type = "pred", terms = c("Vowel", "Gender")) +
  theme_bw() +
  labs(title = "", y = "Predicted duration")
ggsave2(here::here("images", "lmer_vowelf2.png"), width = 4, height = 3)
```


# Overlap

Check density

```{r}
wordplot3 <- function(fdat, vwl1, vwl2){
  plt <- fdat %>% 
    dplyr::rename(label = Vowel)  %>%
    dplyr::filter(label == vwl1 | label == vwl2) %>%
    dplyr::group_by(Word, label) %>%
    dplyr::mutate(meanF2 = mean(lobF2),
                  meanF1 = mean(lobF1)) %>%
    dplyr::ungroup() %>%
    dplyr::group_by(label) %>%
    dplyr::mutate(cF2 = mean(lobF2),
                  cF1 = mean(lobF1)) %>%
    # plot
    ggplot(aes(x = lobF2, y = lobF1)) +
  stat_density_2d(geom = "polygon",
                  aes(alpha = ..level.., fill = label), bins = 8)  +
    facet_grid( ~ type) +
    scale_x_reverse(position = "top") + 
    scale_y_reverse(position = "right") + 
    #scale_fill_distiller(palette = "Blues", direction = 1) +
    geom_text(aes(x = meanF2, y = meanF1, 
                  label = Word, color = label), size = 4) +
    geom_text(aes(x = cF2, y = cF1, 
                  label = vowel), size = 6, color = "gray20") +
    theme_minimal() +
    theme(panel.grid.major = element_blank(), 
                  panel.grid.minor = element_blank(),
                  legend.position = "none") +
  scale_color_manual(values = c("orange3", "gray40")) +
  scale_fill_manual(values = c("orange", "gray")) +
    labs(x = "Formant 2 (Lobanov normalized)", y = "Formant 1 (Lobanov normalized)")
  return(plt)
  }
```


```{r}
pIi <- wordplot3(fdat = bdat, vwl1 = "I", vwl2 = "i")
ggsave(here::here("images", "pIi.png"), height = 3,  width = 5, dpi = 320)
pIi
```


```{r}
wordplot3 <- function(fdat, vwl1, vwl2){
  plt <- fdat %>% 
    dplyr::rename(label = Vowel)  %>%
    dplyr::filter(label == vwl1 | label == vwl2) %>%
    dplyr::group_by(Word, label) %>%
    dplyr::mutate(meanF2 = mean(lobF2),
                  meanF1 = mean(lobF1)) %>%
    dplyr::ungroup() %>%
    dplyr::group_by(label) %>%
    dplyr::mutate(cF2 = mean(lobF2),
                  cF1 = mean(lobF1)) %>%
    # plot
    ggplot(aes(x = lobF2, y = lobF1)) +
  stat_density_2d(geom = "polygon",
                  aes(alpha = ..level.., fill = label), bins = 8)  +
    facet_grid( ~ type) +
    scale_x_reverse(position = "top") + 
    scale_y_reverse(position = "right") + 
    #scale_fill_distiller(palette = "Blues", direction = 1) +
    geom_text(aes(x = meanF2, y = meanF1, 
                  label = Word, color = label), size = 4) +
    geom_text(aes(x = cF2, y = cF1, 
                  label = vowel), size = 6, color = "gray20") +
    theme_minimal() +
    theme(panel.grid.major = element_blank(), 
                  panel.grid.minor = element_blank(),
                  legend.position = "none") +
  scale_color_manual(values = c("red", "darkblue")) +
  scale_fill_manual(values = c("salmon", "lightblue")) +
    labs(x = "Formant 2 (Lobanov normalized)", y = "Formant 1 (Lobanov normalized)")
  return(plt)
  }
```



```{r}
pUu <- wordplot3(fdat = bdat, vwl1 = "U", vwl2 = "u")
ggsave(here::here("images", "pUu.png"), height = 3,  width = 5, dpi = 320)
pUu
```


```{r}
wordplot3 <- function(fdat, vwl1, vwl2){
  plt <- fdat %>% 
    dplyr::rename(label = Vowel)  %>%
    dplyr::filter(label == vwl1 | label == vwl2) %>%
    dplyr::group_by(Word, label) %>%
    dplyr::mutate(meanF2 = mean(lobF2),
                  meanF1 = mean(lobF1)) %>%
    dplyr::ungroup() %>%
    dplyr::group_by(label) %>%
    dplyr::mutate(cF2 = mean(lobF2),
                  cF1 = mean(lobF1)) %>%
    # plot
    ggplot(aes(x = lobF2, y = lobF1)) +
  stat_density_2d(geom = "polygon",
                  aes(alpha = ..level.., fill = label), bins = 8)  +
    facet_grid( ~ type) +
    scale_x_reverse(position = "top") + 
    scale_y_reverse(position = "right") + 
    #scale_fill_distiller(palette = "Blues", direction = 1) +
    geom_text(aes(x = meanF2, y = meanF1, 
                  label = Word, color = label), size = 4) +
    geom_text(aes(x = cF2, y = cF1, 
                  label = vowel), size = 6, color = "gray20") +
    theme_minimal() +
    theme(panel.grid.major = element_blank(), 
                  panel.grid.minor = element_blank(),
                  legend.position = "none") +
  scale_color_manual(values = c("green", "darkorchid4")) +
  scale_fill_manual(values = c("lightgreen", "darkorchid1")) +
    labs(x = "Formant 2 (Lobanov normalized)", y = "Formant 1 (Lobanov normalized)")
  return(plt)
  }
```




```{r}
pEe <- wordplot3(fdat = bdat, vwl1 = "{", vwl2 = "E")
ggsave(here::here("images", "pEe.png"), height = 3,  width = 5, dpi = 320)
pEe
```

# Bhattacharyya's affinity 

function for extracting Bhattacharyya's affinity by type and target variety

```{r bafun, message=F, warning=F}
exba <- function(data, section, target, vwl1, vwl2){
  ba <- data %>%
  ungroup() %>%
  dplyr::filter(type == section,
                TargetVariety == target,
                Vowel == vwl1 | Vowel == vwl2)

  ba_formants <- ba %>%  dplyr::select(lobF1, lobF2)
  # extract vowels
  ba_vowel <- ba %>%  dplyr::select(vowel)
  # spatial data frame
  ba_spdf <- SpatialPointsDataFrame(ba_formants, ba_vowel)
  # calculate Bhattacharyya's affinity
  ba_ba <- kerneloverlap(ba_spdf, method = "BA")
  # result
  return(ba_ba[1,2])
}
```



## I vs i: 


```{r , message=F, warning=F}
# CHN
exba(data = bdat, section = "CHN", target = "AmE", vwl1 = "I", vwl2 = "i")
# ENS
exba(data = bdat, section = "ENS", target = "AmE", vwl1 = "I", vwl2 = "i")
```


## U vs u:

High-back 

```{r , message=F, warning=F}
# CHN
exba(data = bdat, section = "CHN", target = "AmE", vwl1 = "U", vwl2 = "u")
# ENS
exba(data = bdat, section = "ENS", target = "AmE", vwl1 = "U", vwl2 = "u")
```


## E vs {

High-back 

```{r , message=F, warning=F}
# CHN
exba(data = bdat, section = "CHN", target = "AmE", vwl1 = "{", vwl2 = "E")
# ENS
exba(data = bdat, section = "ENS", target = "AmE", vwl1 = "{", vwl2 = "E")
```



# MuPDARF

Prepare data

```{r}
wrds1 <- names(table(nsdtest$Word))[table(nsdtest$Word) > 0]
wrds2 <- names(table(nsdtrain$Word))[table(nsdtrain$Word) > 0]
wrds3 <- names(table(nnsd$Word))[table(nnsd$Word) > 0]
wrds <- Reduce(intersect, list(wrds1, wrds2, wrds3))
# apply to data sets
nsdtest <- nsdtest %>%
  dplyr::mutate(Word = ifelse(Word %in% wrds, Word, "other"))
nsdtrain <- nsdtrain %>%
  dplyr::mutate(Word = ifelse(Word %in% wrds, Word, "other"))
nnsd <- nnsd %>%
  dplyr::mutate(Word = ifelse(Word %in% wrds, Word, "other"))
# inspect
wrds
```


## RF NS

Now, we perform a random forest analysis of the native speaker data.


Now, we perform a random forest analysis of the native speaker data.

```{r l2amp_03_13, message=FALSE, warning=FALSE}
# set seed
set.seed(20230220)
nsrf <- randomForest(Vowel ~ ., data=nsdtrain, ntree=1000, proximity=TRUE, importance=TRUE)
# inspect rf results
nsrf 
```

Next, we plot the results.

```{r l2amp_03_15,  message=FALSE, warning=FALSE}
plot(nsrf)
```

Now, we plot the out-of-bag error frequencies.

```{r l2amp_03_21,  message=FALSE, warning=FALSE}
# plot new precision/error rate
oob.error.data <- data.frame(
  Trees = rep(1:nrow(nsrf$err.rate), times=ncol(nsrf$err.rate)),
  Type = rep(dimnames(nsrf$err.rate)[[2]], each=nrow(nsrf$err.rate)),
  Error = as.vector(unlist(nsrf$err.rate)))
# add vowels
oob.error.data <- oob.error.data %>%
  dplyr::mutate(Vowel = dplyr::case_when(Type == "{" ~ "\u00E6",
                                         Type == "6" ~ "\u0250",
                                         Type == "e" ~ "\u0065",
                                         Type == "E" ~ "\u025B",
                                         Type == "i" ~ "\u0069",
                                         Type == "I" ~ "\u026A",
                                         Type == "Q" ~ "\u0252",
                                         Type == "u" ~ "\u0075",
                                         Type == "U" ~ "\u028A",
                                         Type == "V" ~ "\u028C",
                                         TRUE ~ Type))


ggplot(data=oob.error.data, aes(x=Trees, y=Error)) +
  geom_line(aes(color=Vowel, linetype = Vowel)) +
  theme_bw()
```




```{r}
oob.error.data %>%
  dplyr::filter(Type != "OOB") %>%
  ggplot(aes(x=reorder(Vowel, -Error, mean), y= Error,  group = Vowel)) +                 
  #stat_summary(fun = mean, geom = "point", aes(group= Type)) +          
  #stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2) + 
  geom_boxplot( fill = "lightgray") +
  coord_cartesian(ylim = c(0, 1)) +              
  theme_bw(base_size = 10) +         
  theme(axis.text.x = element_text(size=10),  
        axis.text.y = element_text(size=10, face="plain")) + 
  labs(x = "", y = "Error rate (%)") + 
  scale_y_continuous(limits = c(0.1),
                     labels = seq(0, 100, 20),
                     breaks = seq(0, 1, .2)) +
  scale_color_manual(guide = FALSE)
# save
ggsave(here::here("images", "rf_ns.png"))
```



Now, we check the error rates and accuracy and also check how much the model performs better than a base-line model.

```{r l2amp_03_23,  message=FALSE, warning=FALSE}
# determine accuracy by prediction
# prediction
pnsrf <- predict(nsrf, nsdtest)
# create confusion matrix
confusionMatrix(pnsrf, nsdtest$Vowel)
```



```{r}
cmnsd <- confusionMatrix(pnsrf, nsdtest$Vowel)
# calculate increase in prediction accuracy compared to base-line model
cmnsd$overall[1]
cmnsd$overall[5]

cmnsd$overall[1]/cmnsd$overall[5]
```

Now, we inspect which variables are important for the predictions.


```{r}
impdat <- data.frame(
  Measure = c(rep("Accuracy", length(nsrf$importance[,"MeanDecreaseAccuracy"])),
              rep("Gini", length(nsrf$importance[,"MeanDecreaseGini"]))),
  Label = rep(dimnames(nsrf$importance)[[1]], 2),
  Value = c(nsrf$importance[,"MeanDecreaseAccuracy"], nsrf$importance[,"MeanDecreaseGini"]))
# ordering
impdat <- impdat %>%
  dplyr::group_by(Measure) %>%
  dplyr::mutate(NormMeasure = scale(Value))
# inspect
impdat
```

```{r}
impdat %>%
  ggplot(aes(x = reorder(Label, NormMeasure), y = Value)) +
  geom_point() +
  facet_grid(~Measure, scales="free") +
  coord_flip() +
  theme_bw() +
  labs(x = "", y = "", title = "Importance of Predictors in Random Forest\n (measured as mean decrease if perdictor is absent)")
ggsave(here::here("images", "VarImpRFnsd.png"), width = 6, height = 4)
```


```{r ,  message=FALSE, warning=FALSE}
pred <- as.numeric(predict(nsrf, nsdtest))
pred <- ifelse(pred == 1, 0, 1)
test <- as.numeric(nsdtest$Vowel) 
test <- ifelse(test == 1, 0, 1)
somers2(pred, test) 
```


```{r}
errors_nsd <- nsdtest %>%
  dplyr::mutate(Prediction = predict(nsrf, nsdtest),
                Error = ifelse(Vowel == Prediction, 0, 1)) %>%
  dplyr::group_by(Vowel) %>%
  dplyr::summarise(all = n(),
                   errors = sum(Error),
                   Percent = round(errors/all*100, 1)) %>%
  dplyr::ungroup() %>%
    dplyr::mutate(Vowel = as.character(Vowel),
                  Vowel = dplyr::case_when(Vowel == "{" ~ "\u00E6",
                                         Vowel == "6" ~ "\u0250",
                                         Vowel == "e" ~ "\u0065",
                                         Vowel == "E" ~ "\u025B",
                                         Vowel == "i" ~ "\u0069",
                                         Vowel == "I" ~ "\u026A",
                                         Vowel == "Q" ~ "\u0252",
                                         Vowel == "u" ~ "\u0075",
                                         Vowel == "U" ~ "\u028A",
                                         Vowel == "V" ~ "\u028C",
                                         TRUE ~ Vowel),
                  Type = "ENS") %>%
  dplyr::select(-all, -errors)
# inspect
head(errors_nsd)
```



## RF NNS

Now, we use the random forest analysis of the native speakers to predict how a native speaker would have amplified the adjectives in the non-native speaker data. In a first step, we extract only non-native speaker data.

Next, we use the random forest analysis of the native speakers to predict how a native speaker would have amplified the adjectives.

```{r l2amp_03_45,  message=FALSE, warning=FALSE}
# extract prediction for training data
pnns <- predict(nsrf, nnsd) 
# inspect predictions
head(pnns); head(nnsd$Vowel)  
```

Now, we create a confusion matrix to check the accuracy of the prediction

```{r l2amp_03_49,  message=FALSE, warning=FALSE}
confusionMatrix(pnns, nnsd$Vowel)
```


```{r}
cmnsd <- confusionMatrix(pnns, nnsd$Vowel)
# calculate increase in prediction accuracy compared to base-line model
cmnsd$overall[1]
cmnsd$overall[5]

cmnsd$overall[1]/cmnsd$overall[5]
```

























The prediction accuracy increases by 223.7 percent if use use our model compared to a no information model.

```{r l2amp_03_39,  message=FALSE, warning=FALSE}
pred <- as.numeric(predict(nsrf, nnsd))
pred <- ifelse(pred == 1, 0, 1)
test <- as.numeric(nnsd$Vowel) 
test <- ifelse(test == 1, 0, 1)
somers2(pred, test) 
```


Next, we add the difference between predictions and observed amplification to the data.

```{r l2amp_03_57,  message=FALSE, warning=FALSE}
# add native choice prediction to data
nnsd <- nnsd %>%
  dplyr::mutate(NativeChoice = as.vector(pnns),
                NativeChoice = as.factor(NativeChoice)) %>%
  # code if choice of nns is nativelike or not
  dplyr::mutate(Vowel = as.character(Vowel),
                NativeChoice = as.character(NativeChoice),
                NonNativeLike = ifelse(Vowel == NativeChoice, 0, 1)) %>%
  dplyr::mutate(Vowel = dplyr::case_when(Vowel == "{" ~ "\u00E6",
                                         Vowel == "6" ~ "\u0250",
                                         Vowel == "e" ~ "\u0065",
                                         Vowel == "E" ~ "\u025B",
                                         Vowel == "i" ~ "\u0069",
                                         Vowel == "I" ~ "\u026A",
                                         Vowel == "Q" ~ "\u0252",
                                         Vowel == "u" ~ "\u0075",
                                         Vowel == "U" ~ "\u028A",
                                         Vowel == "V" ~ "\u028C",
                                         TRUE ~ Vowel))

# inspect new data
head(nnsd)
```


```{r}
errors_nnsd <- nnsd %>%
  dplyr::group_by(Vowel, NonNativeLike) %>%
  dplyr::summarise(freq = n()) %>%
  dplyr::ungroup() %>%
  dplyr::group_by(Vowel) %>%
  dplyr::summarise(all = sum(freq),
                   Percent = round(freq/all*100, 1),
                   NonNativeLike = NonNativeLike) %>%
  dplyr::ungroup() %>%
  dplyr::filter(NonNativeLike == 1) %>%
  dplyr::select(-all, -NonNativeLike) %>%
  dplyr::mutate(Type = "CHN")
# inspect
head(errors_nnsd); errors_nsd
```

```{r}
dplyr::full_join(errors_nnsd, errors_nsd) %>%
  dplyr::mutate_if(is.character, factor) %>%
  dplyr::group_by(Vowel) %>%
  dplyr::arrange(Vowel) %>%
  dplyr::mutate(odr = ifelse(Type == "ENS", Percent, NA)) %>%
  tidyr::fill(odr, .direction = "updown") %>%
  dplyr::arrange(-odr) %>%
  dplyr::ungroup() %>%
  ggplot(aes(x = reorder(Vowel, -odr), y = Percent, label = Percent, fill = Type, group = Type)) +
  geom_bar(stat="identity", position = position_dodge()) +
  geom_text(aes(y = Percent+3), position = position_dodge(0.9), size = 2.5, color = "grey10") +
  theme_bw() +
  labs(x = "", y = "Error rate (%)") +
  scale_fill_manual(values = c("gray50", "gray80"),
                    labels = c("L1 English speakers (ENS, test data)", "Chinese learners (CHN)")) +
  theme(legend.position = "top",
        legend.title = element_text(""))
# save
ggsave(here::here("images", "vowelerrors_nns.png"), width = 5, height = 4)
```




```{r}
nnsd %>%
  dplyr::group_by(Vowel) %>%
  dplyr::summarise(Errorrate = mean(as.numeric(NonNativeLike)*100)) %>%
  ggplot(aes(x = reorder(Vowel, -Errorrate), y = Errorrate, label = round(Errorrate, 1))) +
  geom_point() +
  geom_text(vjust=1.3) +
  theme_bw() +
  labs(x = "", y = "Error rate (%)") +
  coord_cartesian(ylim = c(0, 100))
```

```{r}
nnsd %>%
  dplyr::group_by(Vowel) %>%
  dplyr::summarise(Correlation = round(cor(Duration, NonNativeLike), 2)) %>%
  ggplot(aes(x = reorder(Vowel, -Correlation), y = Correlation, label = round(Correlation, 1))) +
  geom_point() +
  geom_text(vjust=1.3) +
  theme_bw() +
  labs(x = "", y = "Correlation (Duration : non native-like production") +
  coord_cartesian(ylim = c(-.5, .5))
```


```{r}
nnsd %>%
  ggplot(aes(x = Vowel, y = Duration)) +
  geom_boxplot() +
  theme_bw() +
  labs(x = "", y = "Duration (sec)")# +
  #coord_cartesian(ylim = c(0, 100))
```

# CIT

```{r eval = F}
# check frequency of Word
pred_nns %>%
  dplyr::group_by(Word) %>%
  dplyr::summarise(freq = n()) %>%
  dplyr::arrange(-freq)
```

prepare data

```{r}
# add proficiency variables
rdat <- cbind(nnsd, pred_nns) %>%
  dplyr::mutate(Proficiency = factor(Proficiency)) %>%
  dplyr::mutate(NonNativeLike = factor(NonNativeLike)) %>%
  dplyr::group_by(Word) %>%
  dplyr::mutate(freq = n()) %>%
  dplyr::ungroup() %>%
  dplyr::mutate(Word = ifelse(freq > 12, as.character(Word), "other"),
                F1 = round(F1, 0),
                F2 = round(F2, 0),
                Duration = round(Duration, 3)) %>%
  dplyr::mutate_if(is.character, factor) %>%
  dplyr::select(-freq, -NativeChoice)
# inspect
str(rdat)
```



```{r}  
# apply bonferroni correction (1 minus alpha multiplied by n of tests (!10 = 3628800))
control = ctree_control(mincriterion = 1-(.05*ncol(rdat)-1))

# create initial conditional inference tree model
citd.ctree <- partykit::ctree(NonNativeLike ~ Vowel + Gender + Age + F1 + F2 + Duration + Proficiency + Frequency,
                    data = rdat, control = ctree_control(mincriterion = 1-(.05/3628800)))
plot(citd.ctree, gp = gpar(fontsize = 7)) # plot final ctree
```


```{r}
# extract p-values
pvals <- unlist(nodeapply(citd.ctree, ids = nodeids(citd.ctree), function(n) info_node(n)$p.value))
pvals <- pvals[pvals <.05]
# plotting
ggparty(citd.ctree) +
  geom_edge(linewidth = .5) +
  geom_edge_label(size = 3) +
  geom_node_label(line_list = list(aes(label = splitvar),
                                   aes(label = paste0("N=", nodesize, ", p", 
                                                      ifelse(pvals < .001, "<.001", paste0("=", round(pvals, 3)))))),
                  line_gpar = list(list(size = 6), 
                                   list(size = 6)), 
                  ids = "inner") +
  geom_node_label(aes(label = paste0("N = ", nodesize)),
    ids = "terminal", nudge_y = -0.0, nudge_x = 0.01, size = 2) +
  geom_node_plot(gglist = list(
    geom_bar(aes(x = "", fill = NonNativeLike),
             position = position_fill(), color = "black"),
      theme_minimal(base_size = 6),
      scale_fill_manual(values = c("gray50", "gray80"), guide = FALSE),
      scale_y_continuous(breaks = c(0, 1)),
    xlab(""), 
    ylab("Probability"),
    geom_text(aes(x = "", group = NonNativeLike,
                  label = after_stat(count)),
              stat = "count", size = 2, 
              position = position_fill(), vjust = 1.1)),
    shared_axis_labels = TRUE)
ggsave2(here::here("images", "cit.png"), width = 12, height = 7)
```


# GLMM


Now, we perform a regression analysis on then difference between native speakers and non-native speakers. We begin by creating fixed-effects intercept-only base-line models.

prepare data

```{r}
# inspect
head(rdat); str(rdat)
```


```{r l2amp_03_59,  message=FALSE, warning=FALSE}
# set options
options(contrasts  =c("contr.treatment", "contr.poly"))
nnsd.dist <- datadist(rdat)
options(datadist = "nnsd.dist")
# generate initial minimal regression model 
# baseline model glm
m0 = glmer(NonNativeLike ~ (1 | Word) + (1 | Speaker), family = binomial, data = rdat) 
# inspect results
summary(m0)
# inspect 
sjPlot::tab_model(m0)
```

Model fitting

```{r}
# wrapper function for linear mixed-models
glmer.glmulti <- function(formula, data, random="",...){
  glmer(paste(deparse(formula),random), family = binomial,  data=data, control = glmerControl(optimizer="bobyqa"), ...)
}
# define formular
form_glmulti = as.formula(paste("NonNativeLike ~  Vowel + Duration +  Gender + Proficiency + WordClass + Frequency"))
```

Extract best 5 models.

```{r eval = F}
library(glmulti)
# multi selection for glmer
mfit <- glmulti(form_glmulti, random="+(1|Word)+(1|Speaker)", 
                data = rdat, method = "h", fitfunc = glmer.glmulti,  includeobjects = T,
                crit = "aic", intercept = TRUE, marginality = FALSE, level = 2)
```



```{r}
# generate final model (include main effects)
mf <- glmer(NonNativeLike ~ (1 | Word) + (1 | Speaker) +
              Duration + Frequency + Vowel:Duration + Vowel:Frequency,
            family = binomial, data = rdat, 
            control = glmerControl(optCtrl=list(maxfun=1e5),
                                   optimizer="bobyqa"))
# inspect 
sjPlot::tab_model(m0, mf)
```


check effects

```{r eval = F}
plot_model(mf, type = "re", sort.est = "sort.all", grid = F) +
  theme_bw() +
  labs(main = "")
ggsave2(here::here("images", "re.png"), width = 6, height = 5)
```

```{r}
sjPlot::plot_model(mf, type = "pred", terms = c("Duration", "Vowel")) +
  theme_bw() +
  coord_cartesian(ylim = c(0, .1))
  labs(title = "", y = "Percent non-target-like production")
ggsave2(here::here("images", "vowel_dur.png"), width = 6, height = 5)
```

```{r}
sjPlot::plot_model(mf, type = "eff", terms = c("Vowel", "Frequency")) +
  theme_bw()+
  labs(title = "", y = "Percent non-target-like production")
ggsave2(here::here("images", "frequency.png"), width = 6, height = 5)
```


# Tabulation

```{r}
# save tables
str(rdat)
str(nsdtrain)
str(nsdtest)
```


# Overview of the data

```{r}
tb1 <- bdat %>%
  dplyr::filter(Vowel == "{"|Vowel == "E"|Vowel == "i"|Vowel == "I"|Vowel == "u"|Vowel == "U")  %>%
  dplyr::mutate(Vowel = dplyr::case_when(Vowel == "{" ~ "\u00E6",
                                         Vowel == "6" ~ "\u0250",
                                         Vowel == "e" ~ "\u0065",
                                         Vowel == "E" ~ "\u025B",
                                         Vowel == "i" ~ "\u0069",
                                         Vowel == "I" ~ "\u026A",
                                         Vowel == "Q" ~ "\u0252",
                                         Vowel == "u" ~ "\u0075",
                                         Vowel == "U" ~ "\u028A",
                                         Vowel == "V" ~ "\u028C",
                                         TRUE ~ Vowel))  %>%
  dplyr::ungroup() %>%
  dplyr::group_by(type) %>%
  dplyr::mutate(speakers = length(table(Speaker))) %>%
  dplyr::ungroup() %>%
  dplyr::group_by(type, Vowel) %>%
  dplyr::summarise(speakers = speakers,
                   obs = n()) %>%
  unique() %>%
  tidyr::spread(Vowel, obs) %>%
  dplyr::ungroup()  %>%
  adorn_totals("row")%>%
  adorn_totals("col") %>%
  dplyr::mutate(Total = Total-speakers)
# save
write.table(tb1, here::here("tables", "tb1_icame.txt"), sep = "\t")
# inspect
tb1
```



tabulate proficiency


```{r}
tb3 <- bdat %>%
  dplyr::ungroup() %>%
  dplyr::filter(type == "CHN")%>%
  dplyr::group_by(Proficiency, Gender) %>%
  dplyr::summarise(speakers = length(table(Speaker))) %>%
  tidyr::spread(Proficiency, speakers) %>%
  dplyr::ungroup()  %>%
  adorn_totals("row")%>%
  adorn_totals("col")
# save
write.table(tb3, here::here("tables", "tb3_icame.txt"), sep = "\t")
# inspect
tb3
```

```{r}
summary(bdat)
```

```{r}
summary(nnsd)
```

```{r}
summary(rdat)
```




# Outro

```{r}
# save tables
base::saveRDS(rdat, file = here::here("tables", "rdat.rda"))
base::saveRDS(nsdtrain, file = here::here("tables", "nsdtrain.rda"))
base::saveRDS(nsdtest, file = here::here("tables", "nsdtest.rda"))
```


# Citation & Session Info

Schweinberger, Martin and Ruihua Yin. 2023. A Corpus-Based Acoustic Analysis of Monophthongal Vowels among Chinese Learners and Native Speakers of English. Brisbane: The University of Queensland, School of Languages and Cultures. 


```{r}
sessionInfo()
```
